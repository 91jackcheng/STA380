---
title: "Homework1"
author: "Jack Cheng"
date: "8/6/2017"
output:
  pdf_document: default
  html_document: default
always_allow_html: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is the homework 1 for the second part of STA380 in Red McCombs business school.



# Statistics Questions:

## Question 1 


From the question we know that:

P(RC)=0.3

P(TC)=1-P(RC)=0.7 since TC is the complement of RC

P(Y)=0.65

P(Y|RC)=0.5

Where RC denotes that the clicker is a random clicker, TC denotes the clicker is a truthful clicker and Y denotes the result is yes.

And we want to know P(Y|TC).

Solution:

P(Y,RC)=P(Y|RC)\*P(RC)=0.5\*0.3=0.15

P(Y,TC)=P(Y)-P(Y,RC)=0.65-0.15=0.5 since TC is the complement of RC

so P(Y|TC)=P(Y,TC)/P(TC)=0.5/0.7=0.7142857



## Question 2

From the question we know that:

P(P|D)=0.993

P(N|Dc)=0.9999

P(D)=0.000025

Where D denotes with desease, Dc denotes no desease, P denotes positive and N denotes negative.

We want to know: P(D|P)

Solution:

since we know Dc is the complement of D 

so P(Dc)=1-P(D)=0.999975 and P(P)=P(Dc,P)+P(D,P)

and N is the complement of P 

so P(P|Dc)=1-P(N|Dc)=0.0001


P(D|P)=P(D,P)/P(P)

=(P(P|D)\*P(D))/(P(D,P)+P(Dc,P))

=(P(P|D)\*P(D))/(P(P|D)\*P(D)+P(Dc,P)\*P(Dc))

=0.993\*0.000025/(0.993\*0.000025+0.0001\*0.999975)=0.1988824

Which is really high! 

That is to say though the sensitivity and specificity of the test is really good, due to the fact that the prior probability of desease is so low as 0.000025, the false positive rate is still really high. This kind of implementing a universal testing policy for the disease will lead to panic and chaos.



# Exploratory analysis: green buildings


# Bootstrapping

```{R}
library(mosaic)
library(quantmod)
library(foreach)
mystocks = c("SPY", "TLT", "LQD","EEM","VNQ")
getSymbols(mystocks)

```

```{R}

EEMa = adjustOHLC(EEM)
LQDa = adjustOHLC(LQD)
SPYa = adjustOHLC(SPY)
TLTa = adjustOHLC(TLT)
VNQa = adjustOHLC(VNQ)

all_returns = cbind(ClCl(EEMa),ClCl(LQDa),ClCl(SPYa),ClCl(TLTa),ClCl(VNQa))
all_returns = as.matrix(na.omit(all_returns))
par(mfrow=c(2,3))
plot(all_returns[,1], type='l',xlab="2007-2017",ylab="Daily Return_EEM")
plot(all_returns[,2], type='l',xlab="2007-2017",ylab="Daily Return_LQD")
plot(all_returns[,3], type='l',xlab="2007-2017",ylab="Market return(S&P)")
plot(all_returns[,4], type='l',xlab="2007-2017",ylab="risk free return")
plot(all_returns[,5], type='l',xlab="2007-2017",ylab="Daily Return_VNQ")

```
First let us just to explore the data by looking at the mean and variance of each asset to get a roughly idea about their risk return properties:
```{R}
boxplot(all_returns,outline=FALSE,col=rainbow(5),ylab='Return')


library(plotly)
plot_ly(type='box', yaxis= list(range = c(-0.5, 0.5))) %>%
  add_boxplot(y = all_returns[,3], name = 'SPY') %>%
  add_boxplot(y = all_returns[,4], name = 'TLT') %>%
  add_boxplot(y = all_returns[,2], name = 'LQD') %>%
  add_boxplot(y = all_returns[,1], name = 'EEM') %>%
  add_boxplot(y = all_returns[,5], name = 'VNQ') %>%
  layout(
    yaxis = list(range = c(-0.5,0.5)))
```

We can see through the graph that EEM and VNQ are with high volatility and rather high return and the TLT is just the most robust way of investing.
Then let us see the sharp ratio for those assets first to get a more through measurement of these assets:

* There are many ways to measure the performance of a certain asset. Here we choose the sharp ratio, Jensen's alpha and treynor ratio as examples.

Note that sharp ratio is the ratio between extra mean return exceed risk free asset and the volitility of asset. 

First use TLT as an approximation to the risk free asset. Calculate the extra return of each asset:
```{R}

EEMa = adjustOHLC(EEM)
LQDa = adjustOHLC(LQD)
SPYa = adjustOHLC(SPY)
TLTa = adjustOHLC(TLT)
VNQa = adjustOHLC(VNQ)
EEM_extra_return=all_returns[,1]-all_returns[,4]
LQD_extra_return=all_returns[,2]-all_returns[,4]
Market_extra_return=all_returns[,3]-all_returns[,4]
VNQ_extra_return=all_returns[,5]-all_returns[,4]

EEM_SD=sd(EEM_extra_return)
LQD_SD=sd(LQD_extra_return)
Market_SD=sd(Market_extra_return)
VNQ_SD=sd(VNQ_extra_return)

SR_EEM=mean(EEM_extra_return)/EEM_SD
SR_LQD=mean(LQD_extra_return)/LQD_SD
SR_Market=mean(Market_extra_return)/Market_SD
SR_VNQ=mean(VNQ_extra_return)/VNQ_SD

SR_EEM
SR_LQD
SR_Market
SR_VNQ
```

Actually here we see that LQD have a lower mean return than risk free asset, that probably suggest us not to invest in this asset since this performance is really terrible. EEM got the highest sharp ratio.


Then we look at Jensen's alpha:

Jensen's alpha is a measurement of the return after adjusting by taking risk into account.

Fit the data with a CAPM model first:

```{R}
lmEEM=lm(EEM_extra_return~Market_extra_return)
lmLQD=lm(LQD_extra_return~Market_extra_return)
lmVNQ=lm(VNQ_extra_return~Market_extra_return)
coef(lmEEM)
coef(lmLQD)
coef(lmVNQ)
```
The alpha and beta are intercept and beta in this particular case.

We can see that EEM get the highest alpha while VNQ get the lowest.

Then let us look at the treynor ratio:
```{R}
TR_EEM=mean(EEM_extra_return)/coef(lmEEM)[2]
TR_LQD=mean(LQD_extra_return)/coef(lmLQD)[2]
TR_VNQ=mean(VNQ_extra_return)/coef(lmVNQ)[2]

TR_EEM
TR_LQD
TR_VNQ
```
We can see here that EEM is with the highest treynor ratio.


Now we begin the boostrapping simulation.

## The even split one
```{R}
set.seed(888)
initial_wealth = 10000
sim1 = foreach(i=1:500, .combine='rbind') %do% {
	total_wealth = initial_wealth
	weights = c(0.2, 0.2, 0.2, 0.2, 0.2)
	holdings = weights * total_wealth
	n_days = 20
	wealthtracker = rep(0, n_days)
	for(today in 1:n_days) {
		return.today = resample(all_returns, 1, orig.ids=FALSE)
		holdings = holdings + holdings*return.today
		total_wealth = sum(holdings)
		wealthtracker[today] = total_wealth
	}
	wealthtracker
}
mean(sim1[,n_days]- initial_wealth)
hist(main = 'The even split portfolio return',sim1[,n_days]- initial_wealth, breaks=30)


```
## The safer portfolio
Next we have our safe portfolio as we invest more on the TLT and since we analysis that LQD is inefficient so we do not invest in it.


```{R}
set.seed(888)
initial_wealth = 10000
sim2 = foreach(i=1:500, .combine='rbind') %do% {
	total_wealth = initial_wealth
	weights = c(0.1, 0, 0.1, 0.7, 0.1)
	holdings = weights * total_wealth
	n_days = 20
	wealthtracker = rep(0, n_days)
	for(today in 1:n_days) {
		return.today = resample(all_returns, 1, orig.ids=FALSE)
		holdings = holdings + holdings*return.today
		total_wealth = sum(holdings)
		wealthtracker[today] = total_wealth
	}
	wealthtracker
}
mean(sim2[,n_days]- initial_wealth)
hist(main = 'The safer portfolio return',sim2[,n_days]- initial_wealth, breaks=30)

```


## The more aggressive one. 

In this one we choose to invest in more EEM which is a more risky asset.

```{R}
set.seed(888)
initial_wealth = 10000
sim3 = foreach(i=1:500, .combine='rbind') %do% {
	total_wealth = initial_wealth
	weights = c(0.6, 0, 0.1, 0.1, 0.1)
	holdings = weights * total_wealth
	n_days = 20
	wealthtracker = rep(0, n_days)
	for(today in 1:n_days) {
		return.today = resample(all_returns, 1, orig.ids=FALSE)
		holdings = holdings + holdings*return.today
		total_wealth = sum(holdings)
		wealthtracker[today] = total_wealth
	}
	wealthtracker
}
mean(sim3[,n_days]- initial_wealth)
hist(main = 'The more aggressive portfolio return',sim3[,n_days]- initial_wealth, breaks=30)
```
Now let us look at the value at risk at 5% level of these portfolios:

```{R}
quantile(sim1[,n_days], 0.05) - initial_wealth
quantile(sim2[,n_days], 0.05) - initial_wealth
quantile(sim3[,n_days], 0.05) - initial_wealth
```
As we can see, here we have the value at risk of 5% for these three different portfolio with different style, and these numbers make sence as the aggressive one have the highest risk and the safe one is with the lowest risk.


#Market segmentation

```{r message=FALSE}
library(pander)
library(ggplot2)
library(LICORS)
library(foreach)
library(mosaic)
library(gridExtra)
library(wordcloud)
```

##Data Cleaning

```{r}
sm = read.csv("~/Desktop/UT Austin/Predictive learning/social_marketing.csv")
sm_1 = subset(sm, select = -c(X,uncategorized, spam, adult, chatter))
sm_sub=sm_1/rowSums(sm_1)
sm_new = scale(sm_sub, center=TRUE, scale=TRUE)
```

##K-means Clustering

```{r}

list= rep(NA, dim(sm_new)[2]-1)
list2= rep(NA, dim(sm_new)[2]-1)

set.seed(1)
for ( i in 2:dim(sm_new)[2]){
  list[i-1]=kmeans(sm_new, i , nstart = 50)$betweenss/ kmeans(sm_new, i , nstart = 50)$tot.withinss *(dim(sm_new)[1]-i)/(i-1)
  list2[i-1]=kmeans(sm_new, i , nstart = 50)$tot.withinss
}

par(mfrow=c(1,2))
plot(list ~ c(2:32), type='b', xlab = 'number of K', ylab = 'CH(K)')
plot(list2 ~ c(2:32), type='b',xlab='number of k',ylab='W(k)')

```
So based on the graphs above, we are able to see the relationship between number of K and CH(K) and W(K) respectively. As the number of K increases, both values of CH(K) and W(K) decrease. Since we could only have 32 clusters for 32 variables, we tried different K from k=2 to k=32 and find out that CH(K) and W(K) are at minimum when K=32. However, this does not make sense to us since K=32 is too large and we would have same variables in different clusters.


## K means for k=2 to k=8
```{r}
set.seed(1)
kmeans_sm2<- kmeans(sm_new, 2, nstart = 50)
print(apply(kmeans_sm2$centers,1,function(x) colnames(sm_new)[order(x, decreasing=TRUE)[1:10]]))
kmeans_sm2$size
```
```{r}
set.seed(1)
kmeans_sm3<- kmeans(sm_new, 3, nstart = 50)
print(apply(kmeans_sm3$centers,1,function(x) colnames(sm_new)[order(x, decreasing=TRUE)[1:10]]))
kmeans_sm3$size
```
```{r}
set.seed(1)
kmeans_sm4<- kmeans(sm_new, 4, nstart = 50)
print(apply(kmeans_sm4$centers,1,function(x) colnames(sm_new)[order(x, decreasing=TRUE)[1:10]]))
kmeans_sm4$size
```
```{r}
set.seed(1)
kmeans_sm5<- kmeans(sm_new, 5, nstart = 50)
print(apply(kmeans_sm5$centers,1,function(x) colnames(sm_new)[order(x, decreasing=TRUE)[1:10]]))
kmeans_sm5$size
```
```{r}
set.seed(1)
kmeans_sm6<- kmeans(sm_new, 6, nstart = 50)
print(apply(kmeans_sm6$centers,1,function(x) colnames(sm_new)[order(x, decreasing=TRUE)[1:10]]))
kmeans_sm6$size
```
```{r}
set.seed(1)
kmeans_sm7<- kmeans(sm_new, 7, nstart = 50)
print(apply(kmeans_sm7$centers,1,function(x) colnames(sm_new)[order(x, decreasing=TRUE)[1:10]]))
kmeans_sm7$size
```

```{r}
set.seed(1)
kmeans_sm8<- kmeans(sm_new, 8, nstart = 50)
print(apply(kmeans_sm8$centers,1,function(x) colnames(sm_new)[order(x, decreasing=TRUE)[1:10]]))
kmeans_sm8$size
```
## K means ++ for k=2 to k=8
```{r}
set.seed(1)
kmeanspp_sm2<- kmeanspp(sm_new, 2)
print(apply(kmeanspp_sm2$centers,1,function(x) colnames(sm_new)[order(x, decreasing=TRUE)[1:10]]))
kmeanspp_sm2$size
```

```{r}
set.seed(1)
kmeanspp_sm3<- kmeanspp(sm_new, 3)
print(apply(kmeanspp_sm3$centers,1,function(x) colnames(sm_new)[order(x, decreasing=TRUE)[1:10]]))
kmeanspp_sm3$size
```

```{r}
set.seed(1)
kmeanspp_sm4<- kmeanspp(sm_new, 4)
print(apply(kmeanspp_sm4$centers,1,function(x) colnames(sm_new)[order(x, decreasing=TRUE)[1:10]]))
kmeanspp_sm4$size
```

```{r}
set.seed(1)
kmeanspp_sm5<- kmeanspp(sm_new, 5)
print(apply(kmeanspp_sm5$centers,1,function(x) colnames(sm_new)[order(x, decreasing=TRUE)[1:10]]))
kmeanspp_sm5$size
```

```{r}
set.seed(1)
kmeanspp_sm6<- kmeanspp(sm_new, 6)
print(apply(kmeanspp_sm6$centers,1,function(x) colnames(sm_new)[order(x, decreasing=TRUE)[1:10]]))
kmeanspp_sm6$size
```

```{r}
set.seed(1)
kmeanspp_sm7<- kmeanspp(sm_new, 7)
print(apply(kmeanspp_sm7$centers,1,function(x) colnames(sm_new)[order(x, decreasing=TRUE)[1:10]]))
kmeanspp_sm7$size
```

```{r}
set.seed(1)
kmeanspp_sm8<- kmeanspp(sm_new, 8)
print(apply(kmeanspp_sm8$centers,1,function(x) colnames(sm_new)[order(x, decreasing=TRUE)[1:10]]))
kmeanspp_sm8$size

```
After we look at the k mean ++ method, we concluded that when k = 5, the clustering result makes more sense to us. So we decided to go with this result.
```{r}

sum(kmeanspp_sm2$withinss)
sum(kmeanspp_sm3$withinss)
sum(kmeanspp_sm4$withinss)
sum(kmeanspp_sm5$withinss)
sum(kmeanspp_sm6$withinss)
sum(kmeanspp_sm7$withinss)
sum(kmeanspp_sm8$withinss)

```

```{r}
sm_distance_matrix = dist(sm_new, method='euclidean')

hier_sm = hclust(sm_distance_matrix, method='average')
cluster1 = cutree(hier_sm, k=5)

hier_sm2 = hclust(sm_distance_matrix, method='complete')
cluster2 = cutree(hier_sm2, k=5)

hier_sm3 = hclust(sm_distance_matrix, method='single')
cluster3 = cutree(hier_sm3, k=5)
summary(factor(cluster1))

summary(factor(cluster2))

summary(factor(cluster3))
```
We tried hcluster for k=5 for min, max and average methods and find out that hcluster does not work very well in this case because most of the data points lie on cluster 1.

So we just give up using this hcluster model due to its significantly inbalance cluster result that obviously will not work for the user segmentation task.

```{r}
plot(hier_sm, cex=0.3)

plot(hier_sm2, cex=0.3)

plot(hier_sm3, cex=0.3)

```

```{R}
set.seed(1)
require("RColorBrewer")
wordcloud(colnames(sm_sub),kmeanspp_sm5$centers[1,],min.freq=0,max.words=1000, colors = rainbow(32),ordered.colors = TRUE) 
wordcloud(colnames(sm_sub),kmeanspp_sm5$centers[2,],min.freq=0,max.words=1000, colors = rainbow(32),ordered.colors = TRUE) 
wordcloud(colnames(sm_sub),kmeanspp_sm5$centers[3,],min.freq=0,max.words=1000, colors = rainbow(32),ordered.colors = TRUE) 
wordcloud(colnames(sm_sub),kmeanspp_sm5$centers[4,],min.freq=0,max.words=1000, colors = rainbow(32),ordered.colors = TRUE) 
wordcloud(colnames(sm_sub),kmeanspp_sm5$centers[5,],min.freq=0,max.words=1000, colors = rainbow(32),ordered.colors = TRUE) 
```

