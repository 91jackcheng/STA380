---
title: "STA 380 Homework2"
author: "Boying You, Daxi Cheng, Jianjie Zheng, Lufang Liu, Yixuan Du"
output:
  markdown: github_document
  html_notebook: default
  pdf_document:
    latex_engine: xelatex
mainfont: Arial
---
```{r setup, cache = T,include = FALSE}
knitr::opts_chunk$set(error = TRUE)
```

```{r global_options, include = FALSE}
knitr::opts_chunk$set(message=FALSE, 
tidy.opts=list(width.cutoff=60)) 
```

This is the homework 2 for the second part of STA380 in Red McCombs business school.

# Question 1 Flights at ABIA

### Goal: Our team decided to answer the following question: when is the best time in a year to fly from Austin to other cities?

### Data cleaning

```{r}
library(ggplot2)
ABIA = read.csv("~/downloads/ABIA.csv")
```

As a first step, we did a four-step data cleaningï¼š dropped all the missing values in *ArrDelay* and *DepDelay*, converted all months and day of week numbers into factors with names, selected only filghts originated from Austin,  and lastly summed the departure delay time and the arrival delay time together to become the total delay time. 

```{r}
# We dropped missing values and converted months and days into factors with month and day of week names. We only kept flights originated from Austin.

ABIA_cleaned = ABIA[!is.na(ABIA$ArrDelay),][!is.na(ABIA$DepDelay),][ABIA$Origin=='AUS',]
ABIA_cleaned$Month<-factor(ABIA_cleaned$Month,levels=c(1,2,3,4,5,6,7,8,9,10,11,12),labels=c("Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"))
ABIA_cleaned$DayOfWeek<-factor(ABIA_cleaned$DayOfWeek,levels=c(1,2,3,4,5,6,7),labels=c("Mon","Tue","Wed","Thu","Fri","Sat","Sun"))

# For these remaining flights, we summed the departure and arrival delay times together to get the total delay time. We believe the total delay time is the one which travelers care the most.

ABIA_cleaned$TotDelay=ABIA_cleaned$ArrDelay+ABIA_cleaned$DepDelay

```

### Analysis by month

We computed and plotted the average total delay time in each month

```{r}
month_delay=ABIA_cleaned[,c('Month','TotDelay','DayOfWeek')]
month_avg_delay = aggregate(.~Month, data=month_delay, mean, na.rm=TRUE, na.action=NULL)
ggplot(data = month_avg_delay,mapping = aes(x= Month, y= TotDelay )) + geom_col()

```

As can be seen from the graphs above, March, June, and December are three months with highest average total delay time. We would recommend you to avoid travelling in these months if possible. 

### Analysis by the day of week in each month

However, we understand that it is often inconvenient to make such dramatic change to your travel plan. Our team wanted to know if it is possible to change the day of the week to travel in those months with highest average total delay time. So we did a further analysis on average delay time in those three months based on different days of a week.

```{r}
new = aggregate(month_delay$TotDelay, by=list(Month=month_delay$Month, DayOfWeek=month_delay$DayOfWeek), FUN=mean)

list= c("Mar","Jun","Dec")

par(mfrow=c(1,3))
for (i in 1:length(list)){
  mon=new[which(new[,1]==list[i]),]
  monthstr=list[i]
  xlab=paste('Day of the Week in',monthstr)
  print(ggplot(data = mon,mapping = aes(x= DayOfWeek, y= x))+geom_col()+labs(x=xlab, y='Average Delay Time'))
  
}


```

As can be seen from the plots above, a good choice is to travel on Wednesday since it has the lowest average delay time in a week. 


### Analysis by distance 

My team also wanted to test if there is a difference between short and long distance flights across all 12 months.It is commonly known that it is easier for long distance (more than 750 miles) flights to re-accommodate their time by changing their speed. This kind of re-accommodating may require certain traffic and weather conditions that are related to month.

```{r}
LongFlight=ABIA_cleaned[which(ABIA_cleaned$Distance>=750),c('Month','TotDelay','DayOfWeek')]
ShortFlight=ABIA_cleaned[which(ABIA_cleaned$Distance<750),c('Month','TotDelay','DayOfWeek')]



LongFlight_month = aggregate(.~Month, data=LongFlight, mean, na.rm=TRUE, na.action=NULL)
ShortFlight_month = aggregate(.~Month, data=ShortFlight, mean, na.rm=TRUE, na.action=NULL)

LongFlight_month$LongFlightSave=LongFlight_month$TotDelay-ShortFlight_month$TotDelay

LongFlight_month[which(LongFlight_month$LongFlightSave>=0),c(1,4)]
```

So for the months listed above, we recommend taking a long distance trip in order to minimize the delay. As for the rest of the year, a short distance trip would be better.

### Conclusion 

Given the analysis above, our team offer these suggestions to the travelers in Austin:

* If possible, schedule your trip in September, October and December to avoid the heavy traffic and long delay time.

* If you have to travel among those months, Wednesday would be recommended for the sake of attaining relatively lower delay time.

* One additional suggestion: March, May, and October are the ideal months for short trips, and also, the rest months of the year are ideal months for longer trips.


# Question 2 Author attribution

For this question, we decided to pick Naive Bayes and Random Forest methods because they are considered as common ways to deal with text mining problems. 
After we performed these methods, we would compare them using prediction accuracies in the test data. 
Our goal in this question is to perform these two methods and use a better method to predict the author names given in the testing data.

### Naive Bayes method 

```{r cars}
library(tm)
```

We defined a function called readerPlain to read the content of text files.

```{r}
readerPlain = function(fname){
  readPlain(elem=list(content=readLines(fname)),id=fname, language='en') }
```

At first, we loaded the training and testing directory.

```{r}
train_dirs = Sys.glob("~/downloads/STA380-master/data/ReutersC50/C50train/*")
test_dirs = Sys.glob("~/downloads/STA380-master/data/ReutersC50/C50test/*")
```

#### The Sparse matrix

For training set

```{r}
file_list_train = NULL
labels_train = NULL
y_train = NULL
for(author in train_dirs) {
  author_name = tail(strsplit(author,split="/")[[1]],1)
  files_to_add = Sys.glob(paste0(author, '/*.txt'))
  file_list_train = append(file_list_train, files_to_add)
  labels_train = append(labels_train, rep(author_name, length(files_to_add)))
}
```

By using the for loop here, we got the author name for each txt files in the training data set and also the file path toward each text files for each author in the training set.

We then combined all the text files in the training set and made it into corpus by using Corpus() function.

```{r}
train_docs = lapply(file_list_train, readerPlain) 
names(train_docs) = file_list_train
names(train_docs) = sub('.txt', '', names(train_docs))
my_corpus_train = Corpus(VectorSource(train_docs))
```

For the testing set, we repeated what we did for the training set and made a corpus for the testing data.

```{r}
file_list_test = NULL
labels_test = NULL
for(author in test_dirs) {
  author_name = tail(strsplit(author,split="/")[[1]],1)
  files_to_add = Sys.glob(paste0(author, '/*.txt'))
  file_list_test = append(file_list_test, files_to_add)
  labels_test = append(labels_test, rep(author_name, length(files_to_add)))
}

test_docs = lapply(file_list_test, readerPlain) 
names(test_docs) = file_list_test
names(test_docs) = sub('.txt', '', names(test_docs))
my_corpus_test = Corpus(VectorSource(test_docs))

```

#### Data Preprocessing

For train data

```{r}

my_corpus_train = tm_map(my_corpus_train, content_transformer(tolower)) 
# make everything lowercase
my_corpus_train = tm_map(my_corpus_train, content_transformer(removeNumbers)) 
# remove numbers
my_corpus_train = tm_map(my_corpus_train, content_transformer(removePunctuation)) 
# remove punctuation
my_corpus_train = tm_map(my_corpus_train, content_transformer(stripWhitespace)) 
# remove excess white-space
my_corpus_train = tm_map(my_corpus_train, content_transformer(removeWords), stopwords("SMART"))
```

After we removed numbers, punctuations, excess white-spaces and stopwords, we got our new corpus for the training set.

```{r}
# for test data
my_corpus_test = tm_map(my_corpus_test, content_transformer(tolower)) # make everything lowercase
my_corpus_test = tm_map(my_corpus_test, content_transformer(removeNumbers)) # remove numbers
my_corpus_test = tm_map(my_corpus_test, content_transformer(removePunctuation)) # remove punctuation
my_corpus_test = tm_map(my_corpus_test, content_transformer(stripWhitespace)) ## remove excess white-space
my_corpus_test = tm_map(my_corpus_test, content_transformer(removeWords), stopwords("SMART"))
```

#### Model Prediction and Accuracy

```{r}
library('naivebayes')
```
Then We made our training and testing corpus into document term matrices.
```{r}
DTM_train = DocumentTermMatrix(my_corpus_train)
DTM_test = DocumentTermMatrix(my_corpus_test)
```

By using class functions, we finally got our sparse matrix for the training and testing sets. In addition, inspect function were used here to get access to the values inside of the sparse matrix.

```{r}
class(DTM_train)  
class(DTM_test)

inspect(DTM_train[1:10,1:20])
inspect(DTM_test[1:10,1:20])
```

We then went on removing some sparse terms in our sparse matrix for the training and testing set. After that, we converted our sparse matrices to word frequency matrices for the training and testing sets in data frame type.

```{r}
DTM_train = removeSparseTerms(DTM_train, 0.975)
DTM_test = removeSparseTerms(DTM_test, 0.975)

X_train = as.data.frame(as.matrix(DTM_train))
X_test = as.data.frame(as.matrix(DTM_test))
```

Now we tried to get an intersect of common columns that present in both training and testing sets.


We created a new training set which only contains common columns from both training and testing sets.

```{r}
common_cols = intersect(names(X_train), names(X_test))
X_train_2 =X_train[,c(common_cols)]

```

We then built the naive bayes model using the new training set and then found out its model accuracy.

```{r}

nb_train = naive_bayes(x=X_train_2, y= as.factor(labels_train),laplace=1)
train.pred = predict(nb_train, X_test)

count=0
for (i in 1:2500){
  if(train.pred[i]==labels_train[i]){
    count=count+1
  }
}
accuracy = count/2500
cat('Prediction accuracy for navie bayes method is ',accuracy)
```

Finally, We got an accuracy of 18.28% when we used the naive bayes model to predict the author names in test set.  


We noticed that we got a rather low accuracy rate using Naive Bayes. The main reason here is that the text files actually violate the assumption of independence features likelihood assumed in Naive Bayes algorithm. The probability of which word chosen in the text file is strongly related to which other words have already been chosen. 
For the reasons above, we decided to use a random forest model to get a better accuracy rate.

### Random Forest Method

We used the new training data set got from the previous section to perform the randomforest model with tree number equalts to 100.

```{r}
library(randomForest)
set.seed(1)
rfmodel <- randomForest(x=X_train_2,y=factor(labels_train),ntree=100)
rf.pred = predict(rfmodel,newdata=X_test)
conf_matrix = table(rf.pred,labels_train)
```

Calculated the number of corrected predictions through all text files.

```{r}
count = 0
for(i in 1:dim(conf_matrix)[1]){
  count = count + conf_matrix[i,i]
}

cat('Prediction accuracy for Random Forest method is around', count/2500)
```

We used 4 different values(50,100,150,200) to figure out the best tree number for the random forest model and finally came out with our highest prediction with ntree=100 and the corresponding accuracy around 60%.


# Question 3 Grocery (Association Rule Mining)

### Overview: 

In this question, the main goal is to find interesting association rules for shopping baskets. The key is to pick thresholds for lift and confidence. Here, we define "interesting" rules as rules that could be used for solving problemsv in business settings.

### Dataset Loading and Initializing:

We first read in the given groceries dataset and created a transactions object using the "read.transactions" function in R. The object format satisfied the format expected by the "arules" package. We then inspected and verified the first 10 items in this object. Next, we did a summary statistics on the object created and plotted the frequency of each food item.

```{r}
# Load the libraries
library(arules)
library(arulesViz)
```

```{r, warning=FALSE}
# Read in the text file as a format accessible for "arules" package
library(arules)
grocery <- read.transactions('https://raw.githubusercontent.com/jgscott/STA380/master/data/groceries.txt', sep=',')
summary(grocery)
inspect(grocery[1:10])
```

```{r, warning=FALSE}
# Plot top 10 frequent appearing items in grocery
itemFrequencyPlot(grocery,topN=10)
```


### Apriori Algorithm Applying and Parameters Selecting

Support is the fraction of which our item set occurs in our dataset. Therefore, we chose a relatively small support ratio to have more rules included for inspection.

Confidence is the probability that a rule is correct for a new transaction with items on the left. We set the minimum confidence to be 0.4 which we believe is moderate. Then we sorted rules by confidence and found the top10-ranked rules are mostly predicting "whole milk" with 100% confidence ratio, which makes sense as whole milk is the most common item for all shoppers.

Lift is the ratio by which by the confidence of a rule exceeds the expected confidence. Based on lift ratio, we sorted the rules again. We found top10-ranked rules all make common sense. For example, first rule says: with bottled beer and liquor in lhs, you will likely see red/blush wine in rhs.

```{r, warning=FALSE}
groceryrule1 <- apriori(grocery,parameter=list(support=0.001, confidence=0.4, maxlen=10))

groceryrule1_confidence <- sort(groceryrule1, by="confidence", decreasing=TRUE)
inspect(groceryrule1_confidence[1:10])
summary (groceryrule1)

groceryrule1_lift <- sort(groceryrule1, by="lift", decreasing=TRUE)
inspect(groceryrule1_lift[1:10])
summary (groceryrule1)
```


### Exploration using threholds

There are mainly three objective measures: support, confidence and lift.

```{r, warning=FALSE}
# Generally explore rules
# Choose subset according to certain lift and confidence threholds (we use their mean in this case)
groceryrules <- apriori(grocery, parameter = list(support = 0.001, confidence = 0.5))
# Show the top 10 rules
inspect(groceryrules[1:10])
inspect(subset(groceryrules,subset=lift>3.262)[1:5]) 
inspect(subset(groceryrules, subset=confidence > 0.6250)[1:5])
plot(head(subset(groceryrules,subset=lift>3.262), 20), method = "graph", control=list(cex=.8))
plot(head(subset(groceryrules, subset=confidence > 0.6250), 20), method = "graph", control=list(cex=.8))
```

Observation: 

In the case of lift is higher than its mean, there are some associations related to whole milk. Lift could be thought as how much more likely an item is to be purchased given that it is known that another item has been purchased relative to its general purchase rate. For example, with rice and sugar,it is almost four times more likely that whole milk is going to be purchased than in the general grocery purchase.


In the case of confidence is higher than its mean, there are also some associations related to whole milk.Confidence represents how likely a rule is. For example,rice,sugar associated with whole milk is a rule that has a confidence "1".


Interpretation and discussion:
In this case, it is their means that are as threholds because we targeted rules with above_than_average threholds level. 


Pontential application and suggestions:
Associations that could be used are like rice, sugar with wholemilk. Grocery stores could position these three product items closely.

However,as when lift is high, it could be the case that support is low, which means that the itemsets are rare in all grocery transactions.And rules that hold 100% of the time may not have the highest possible lift. As a result, method above has somewhat problmatic.

### Exploration with subjective selection and objective measure 

```{r, warning=FALSE}
inspect(subset(groceryrules, subset=support > 0.01 & confidence > 0.5 & lift>3))
```

Observation: 

After trying different combinations of threholds, we chose the above one. In this case, there are associations of other vegetables with citrus fruit,root vegetables, and with root vegetables,tropical fruit. 


Interpretation and discussion:
As in this case, the corresponding association rule is actionable, we use these threholds.


Pontential application and suggestions:
Associations that could be used are like other vegetables with citrus fruit,root vegetables, and with root vegetables,tropical fruit. Grocery stores could position these these product items closely.

### Items Targeting

After general exploration, we decided to pick soda as an example to do item targeting.

There are two questions we cared about:

#### 1. What are customers likely to buy before buying soda?

```{r, warning=FALSE}
groceryrule2 <-apriori(data=grocery, parameter=list(support=0.001, confidence=0.4), 
               appearance = list(default="lhs",rhs="soda"),
               control = list(verbose=F))
groceryrule2 <-sort(groceryrule2,by="confidence")
inspect(groceryrule2[1:5])
```

#### 2. What are customers likely to buy if they've purchased soda?

```{r, warning=FALSE}
groceryrules3<-apriori(data=grocery, parameter=list(supp=0.001,conf = 0.15,minlen=2), 
               appearance = list(default="rhs",lhs="soda"),
               control = list(verbose=F))
groceryrules3<-sort(groceryrules3,by="confidence")
inspect(groceryrules3[1:5])
```

These two examples show that these association rules can help store managers to promote the sales of certain goods by placing them closer to other goods that are associated with them.